---
id: chapter4-sections-4-2-exteroception
title: "Exteroception: Lidar, Stereo Vision, RGB-D Cameras"
---

## 4.2 Exteroception: Lidar, Stereo Vision, RGB-D Cameras

Exteroception refers to a robot's ability to sense its external environment. For humanoid robots, this is crucial for navigation, object recognition, human interaction, and environmental mapping. A variety of sensors provide this external awareness, each with unique strengths and applications.

### Lidar (Light Detection and Ranging)

**Lidar** sensors emit pulsed laser light and measure the time it takes for the light to return, calculating distance to objects. They generate highly accurate 2D or 3D point clouds, which are excellent for mapping, obstacle detection, and localization, especially in complex or outdoor environments.

*   **2D Lidar:** Rotates a laser beam in a plane, generating a 2D slice of the environment.
*   **3D Lidar:** Uses multiple lasers or scanning mechanisms to generate dense 3D point clouds.

### Stereo Vision

**Stereo vision** systems mimic human binocular vision, using two cameras separated by a known baseline. By finding corresponding points in the left and right images, the depth (distance) to objects can be calculated through triangulation. This provides depth information along with color images.

```latex
Z = \frac{B f}{d}
```
Where $Z$ is depth, $B$ is baseline, $f$ is focal length, and $d$ is disparity.

### RGB-D Cameras (Red-Green-Blue-Depth)

**RGB-D cameras** (e.g., Microsoft Kinect, Intel RealSense) provide both color (RGB) images and per-pixel depth information. They achieve depth sensing using various technologies, such as structured light or Time-of-Flight (ToF).

*   **Structured Light:** Projects a known pattern onto the scene and analyzes its deformation to calculate depth.
*   **Time-of-Flight (ToF):** Measures the time for light to travel from the camera, reflect off objects, and return.

### Example: Processing a Simulated Point Cloud (Conceptual)

This Python snippet demonstrates basic processing of a simulated LiDAR point cloud, e.g., filtering.

```python
import numpy as np

def process_point_cloud(point_cloud_data, min_distance=0.1, max_distance=10.0):
    # point_cloud_data is assumed to be an Nx3 array (x, y, z)

    # Calculate distances from origin for filtering
    distances = np.linalg.norm(point_cloud_data, axis=1)

    # Filter points by distance
    filtered_cloud = point_cloud_data[(distances > min_distance) & (distances < max_distance)]

    return filtered_cloud

# Simulate a point cloud (e.g., a wall at 2m and some close noise)
simulated_cloud = np.array([
    [0.1, 0.0, 0.0], # Noise
    [2.0, 0.5, 0.0], # Wall point
    [2.0, 0.6, 0.1], # Wall point
    [0.05, 0.0, 0.0], # Closer noise
    [2.0, 0.4, -0.1], # Wall point
    [15.0, 1.0, 0.0] # Far away point
])

processed_cloud = process_point_cloud(simulated_cloud)
print("Original Point Cloud (first 3 points):")
print(simulated_cloud[:3])
print("\nFiltered Point Cloud (showing only points within 0.1m and 10m):")
print(processed_cloud)

# In ROS, point cloud data (from Lidar, RGB-D) is often published on topics
# like `/scan` (2D Lidar) or `/camera/depth/points` (RGB-D). Libraries like `PCL`
# (Point Cloud Library) are used for advanced processing.
# Example command to view point cloud in RViz:
# roslaunch rviz rviz
# Add 'PointCloud2' display and set topic to relevant point cloud topic.
```

### ASCII Diagram: Stereo Vision Principle

```
Left Camera     Baseline     Right Camera
   O-----------<------->------------O
    \                         /
     \                       /
      \                     /
       \                   /
        \                 /
         \               /
          \             /
           \           /
            \         /
             V-------V (Object point)
                 Z (Depth)
```

### Multiple Choice Question

Which type of exteroceptive sensor typically provides both color images and per-pixel depth information?

a) 2D Lidar

b) Stereo Vision

c) RGB-D Camera

d) Force/Torque Sensor

**Answer:** c)

### Short Question

Describe a scenario where a Lidar sensor would be more advantageous than a stereo vision system for a humanoid robot's navigation. Explain why.

### Assignment

Research the concept of *structured light* in RGB-D cameras. Explain how it works to calculate depth and discuss its main advantages and limitations compared to Time-of-Flight (ToF) cameras.
